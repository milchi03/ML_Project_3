{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Classifier as benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this exercise is to get a feeling and understanding on the importance of\n",
    "representation and extraction of information from complex media content, in this case images or\n",
    "text. You will thus get some datasets that have an image classification target.  \n",
    "\n",
    "(1) In the first step, you shall try to find a good classifier with „traditional“ feature extraction\n",
    "methods. Thus, pick one feature extractor based on e.g. Bag Of Words, or n-grams, or similar\n",
    "You shall evaluate them on two shallow algorithms, optimising the parameter settings to see what\n",
    "performance you can achieve, to have a baseline for the subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          iid                                              title  \\\n",
      "0  Fq+C96tcx+  ‘A target on Roe v. Wade ’: Oklahoma bill maki...   \n",
      "1  bHUqK!pgmv  Study: women had to drive 4 times farther afte...   \n",
      "2  4Y4Ubf%aTi        Trump, Clinton clash in dueling DC speeches   \n",
      "3  _CoY89SJ@K  Grand jury in Texas indicts activists behind P...   \n",
      "4  +rJHoRQVLe  As Reproductive Rights Hang In The Balance, De...   \n",
      "\n",
      "                                                text label  \n",
      "0  UPDATE: Gov. Fallin vetoed the bill on Friday....  REAL  \n",
      "1  Ever since Texas laws closed about half of the...  REAL  \n",
      "2  Donald Trump and Hillary Clinton, now at the s...  REAL  \n",
      "3  A Houston grand jury investigating criminal al...  REAL  \n",
      "4  WASHINGTON -- Forty-three years after the Supr...  REAL  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"C:/Users/User/Downloads/fake_and_real_news_dataset.csv\" \n",
    "dataset1 = pd.read_csv(file_path, encoding=\"utf-8\", on_bad_lines='skip')\n",
    "\n",
    "\n",
    "dataset1.columns = ['iid', 'title', 'text', 'label']\n",
    "\n",
    "print(dataset1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1648\\4276140169.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset1['label'].fillna('FAKE', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values in the 'label' column with 'FAKE'\n",
    "dataset1['label'].fillna('FAKE', inplace=True)\n",
    "\n",
    "# Verify that there are no missing values left in the 'label' column\n",
    "print(dataset1['label'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  label  \n",
      "0  December 31, 2017       1  \n",
      "1  December 29, 2017       1  \n",
      "2  December 31, 2017       1  \n",
      "3  December 30, 2017       1  \n",
      "4  December 29, 2017       1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_df = pd.read_csv(\"C:/Users/User/Downloads/archive (14)/True.csv\")\n",
    "fake_df = pd.read_csv(\"C:/Users/User/Downloads/archive (14)/Fake.csv\")\n",
    "\n",
    "# Add label column\n",
    "true_df[\"label\"] = 1  \n",
    "fake_df[\"label\"] = 0  \n",
    "\n",
    "# Combine both datasets\n",
    "dataset2 = pd.concat([true_df, fake_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(dataset2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          iid                                              title  \\\n",
      "0  Fq+C96tcx+  ‘A target on Roe v. Wade ’: Oklahoma bill maki...   \n",
      "1  bHUqK!pgmv  Study: women had to drive 4 times farther afte...   \n",
      "2  4Y4Ubf%aTi        Trump, Clinton clash in dueling DC speeches   \n",
      "3  _CoY89SJ@K  Grand jury in Texas indicts activists behind P...   \n",
      "4  +rJHoRQVLe  As Reproductive Rights Hang In The Balance, De...   \n",
      "\n",
      "                                                text  label  \\\n",
      "0  UPDATE: Gov. Fallin vetoed the bill on Friday....      1   \n",
      "1  Ever since Texas laws closed about half of the...      1   \n",
      "2  Donald Trump and Hillary Clinton, now at the s...      1   \n",
      "3  A Houston grand jury investigating criminal al...      1   \n",
      "4  WASHINGTON -- Forty-three years after the Supr...      1   \n",
      "\n",
      "                                       combined_text  \\\n",
      "0   a target on roe v wade oklahoma bill making i...   \n",
      "1  study women had to drive 4 times farther after...   \n",
      "2  trump clinton clash in dueling dc speeches don...   \n",
      "3  grand jury in texas indicts activists behind p...   \n",
      "4  as reproductive rights hang in the balance deb...   \n",
      "\n",
      "                               combined_text_encoded  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Encode labels: FAKE -> 0, TRUE -> 1\n",
    "dataset1['label'] = dataset1['label'].map({'FAKE': 0, 'REAL': 1})\n",
    "\n",
    "# Combine 'title' and 'text' into one column\n",
    "dataset1['combined_text'] = dataset1['title'] + \" \" + dataset1['text']\n",
    "\n",
    "# Convert text to lowercase\n",
    "dataset1['combined_text'] = dataset1['combined_text'].str.lower()\n",
    "\n",
    "# Remove special characters and punctuation\n",
    "dataset1['combined_text'] = dataset1['combined_text'].apply(lambda x: re.sub(r'\\W+', ' ', str(x)))\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))  # Define stop words\n",
    "\n",
    "# Tokenization, stop-word removal, and stemming\n",
    "dataset1['combined_text_tokens'] = dataset1['combined_text'].apply(word_tokenize)\n",
    "dataset1['combined_text_tokens'] = dataset1['combined_text_tokens'].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
    ")\n",
    "dataset1['combined_text_stemmed'] = dataset1['combined_text_tokens'].apply(\n",
    "    lambda tokens: [ps.stem(token) for token in tokens]\n",
    ")\n",
    "\n",
    "# Convert stemmed tokens back into strings for CountVectorizer\n",
    "dataset1['combined_text_stemmed_text'] = dataset1['combined_text_stemmed'].apply(' '.join)\n",
    "\n",
    "# Use CountVectorizer to convert text into a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(dataset1['combined_text_stemmed_text'])\n",
    "dataset1['combined_text_encoded'] = vector.toarray().tolist()\n",
    "\n",
    "# Drop intermediate columns\n",
    "dataset1 = dataset1.drop(columns=['combined_text_tokens', 'combined_text_stemmed', 'combined_text_stemmed_text'])\n",
    "\n",
    "\n",
    "\n",
    "print(dataset1.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the combined_text_encoded column we can only see zeros we will check if there are non-zero values in order to be sure the preprocessing has gone smoothly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero entries: 1226720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>combined_text_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fq+C96tcx+</td>\n",
       "      <td>‘A target on Roe v. Wade ’: Oklahoma bill maki...</td>\n",
       "      <td>UPDATE: Gov. Fallin vetoed the bill on Friday....</td>\n",
       "      <td>1</td>\n",
       "      <td>a target on roe v wade oklahoma bill making i...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bHUqK!pgmv</td>\n",
       "      <td>Study: women had to drive 4 times farther afte...</td>\n",
       "      <td>Ever since Texas laws closed about half of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>study women had to drive 4 times farther after...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Y4Ubf%aTi</td>\n",
       "      <td>Trump, Clinton clash in dueling DC speeches</td>\n",
       "      <td>Donald Trump and Hillary Clinton, now at the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>trump clinton clash in dueling dc speeches don...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_CoY89SJ@K</td>\n",
       "      <td>Grand jury in Texas indicts activists behind P...</td>\n",
       "      <td>A Houston grand jury investigating criminal al...</td>\n",
       "      <td>1</td>\n",
       "      <td>grand jury in texas indicts activists behind p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+rJHoRQVLe</td>\n",
       "      <td>As Reproductive Rights Hang In The Balance, De...</td>\n",
       "      <td>WASHINGTON -- Forty-three years after the Supr...</td>\n",
       "      <td>1</td>\n",
       "      <td>as reproductive rights hang in the balance deb...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          iid                                              title  \\\n",
       "0  Fq+C96tcx+  ‘A target on Roe v. Wade ’: Oklahoma bill maki...   \n",
       "1  bHUqK!pgmv  Study: women had to drive 4 times farther afte...   \n",
       "2  4Y4Ubf%aTi        Trump, Clinton clash in dueling DC speeches   \n",
       "3  _CoY89SJ@K  Grand jury in Texas indicts activists behind P...   \n",
       "4  +rJHoRQVLe  As Reproductive Rights Hang In The Balance, De...   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  UPDATE: Gov. Fallin vetoed the bill on Friday....      1   \n",
       "1  Ever since Texas laws closed about half of the...      1   \n",
       "2  Donald Trump and Hillary Clinton, now at the s...      1   \n",
       "3  A Houston grand jury investigating criminal al...      1   \n",
       "4  WASHINGTON -- Forty-three years after the Supr...      1   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0   a target on roe v wade oklahoma bill making i...   \n",
       "1  study women had to drive 4 times farther after...   \n",
       "2  trump clinton clash in dueling dc speeches don...   \n",
       "3  grand jury in texas indicts activists behind p...   \n",
       "4  as reproductive rights hang in the balance deb...   \n",
       "\n",
       "                               combined_text_encoded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nonzero_count = vector.nnz  # or X.count_nonzero()\n",
    "print(\"Number of nonzero entries:\", nonzero_count)\n",
    "\n",
    "\n",
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iid                                              title  \\\n",
      "0  AA101  Government announces new economic policies to ...   \n",
      "1  BB202  Scientists discover new species in the Amazon ...   \n",
      "2  CC303  Major cyberattack disrupts banking systems acr...   \n",
      "3  DD404  Famous actor donates millions to climate chang...   \n",
      "4  EE505  Experts warn about rising sea levels affecting...   \n",
      "\n",
      "                                                text  label  \\\n",
      "0  The government has unveiled a series of new ec...      1   \n",
      "1  A group of scientists has identified a previou...      1   \n",
      "2  A large-scale cyberattack has disrupted bankin...      0   \n",
      "3  A world-renowned actor has pledged a significa...      1   \n",
      "4  Climate scientists have issued a warning about...      1   \n",
      "\n",
      "                                       combined_text  \\\n",
      "0  Government announces new economic policies to ...   \n",
      "1  Scientists discover new species in the Amazon ...   \n",
      "2  Major cyberattack disrupts banking systems acr...   \n",
      "3  Famous actor donates millions to climate chang...   \n",
      "4  Experts warn about rising sea levels affecting...   \n",
      "\n",
      "                               combined_text_encoded  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, ...  \n",
      "2  [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
      "4  [2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.DataFrame({\n",
    "    'iid': ['AA101', 'BB202', 'CC303', 'DD404', 'EE505', 'FF606', 'GG707', 'HH808', 'II909', 'JJ010'],\n",
    "    'title': [\n",
    "        'Government announces new economic policies to boost growth',\n",
    "        'Scientists discover new species in the Amazon rainforest',\n",
    "        'Major cyberattack disrupts banking systems across Europe',\n",
    "        'Famous actor donates millions to climate change research',\n",
    "        'Experts warn about rising sea levels affecting coastal cities',\n",
    "        'Stock market hits record high amid economic recovery',\n",
    "        'New study links processed foods to increased health risks',\n",
    "        'Sports team wins championship in dramatic final match',\n",
    "        'Breakthrough in renewable energy technology announced',\n",
    "        'Major corporation accused of environmental violations'\n",
    "    ],\n",
    "    'text': [\n",
    "        'The government has unveiled a series of new economic policies aimed at stimulating growth and increasing employment opportunities. Officials believe these measures will help stabilize the economy.',\n",
    "        'A group of scientists has identified a previously unknown species of amphibians deep in the Amazon rainforest, shedding light on the region’s incredible biodiversity and ecological significance.',\n",
    "        'A large-scale cyberattack has disrupted banking operations across multiple European countries, causing financial institutions to implement emergency security measures to protect customer data.',\n",
    "        'A world-renowned actor has pledged a significant portion of their wealth to support climate change research initiatives, aiming to fund projects that seek solutions to environmental issues.',\n",
    "        'Climate scientists have issued a warning about the rising sea levels and their impact on major coastal cities, urging governments to take immediate action to prevent catastrophic consequences.',\n",
    "        'The stock market has reached an all-time high, fueled by strong corporate earnings and renewed investor confidence in the ongoing economic recovery, according to financial analysts.',\n",
    "        'A newly published study has found a correlation between the consumption of processed foods and increased health risks, leading to calls for better dietary regulations and awareness campaigns.',\n",
    "        'In an intense and thrilling final match, the underdog sports team secured a stunning victory, claiming the championship title and delighting fans around the world with their performance.',\n",
    "        'Scientists have announced a major breakthrough in renewable energy technology, which could significantly improve the efficiency of solar panels and make sustainable energy more accessible globally.',\n",
    "        'A well-known multinational corporation has come under fire after allegations surfaced about environmental violations, prompting an investigation into its practices and potential legal actions.'\n",
    "    ],\n",
    "    'label': ['TRUE', 'TRUE', 'FAKE', 'TRUE', 'TRUE', 'TRUE', 'TRUE', 'TRUE', 'TRUE', 'FAKE'],\n",
    "})\n",
    "\n",
    "# Encode labels: FAKE -> 0, TRUE -> 1\n",
    "dataset1['label'] = dataset1['label'].map({'FAKE': 0, 'TRUE': 1})\n",
    "\n",
    "# Combine 'title' and 'text' columns to create a single text feature\n",
    "dataset1['combined_text'] = dataset1['title'] + \" \" + dataset1['text']\n",
    "\n",
    "# Initialize PorterStemmer for word stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize and stem the combined text to reduce words to their root form\n",
    "dataset1['combined_text_tokens'] = dataset1['combined_text'].apply(word_tokenize)\n",
    "\n",
    "dataset1['combined_text_stemmed'] = dataset1['combined_text_tokens'].apply(lambda tokens: [ps.stem(token) for token in tokens])\n",
    "\n",
    "# Convert stemmed tokens back into strings for CountVectorizer\n",
    "dataset1['combined_text_stemmed_text'] = dataset1['combined_text_stemmed'].apply(' '.join)\n",
    "\n",
    "# Use CountVectorizer to convert text into a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(dataset1['combined_text_stemmed_text'])\n",
    "dataset1['combined_text_encoded'] = vector.toarray().tolist()\n",
    "\n",
    "# Drop intermediate columns to clean up the DataFrame\n",
    "dataset1 = dataset1.drop(columns=['combined_text_tokens', 'combined_text_stemmed', 'combined_text_stemmed_text'])\n",
    "\n",
    "print(dataset1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the 'date' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset2.drop(columns=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset2.isnull().sum())  # Check missing values\n",
    "dataset2 = dataset2.dropna()  # Drop rows with missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning (converting text to lowercase, removing special characters, numbers and punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "dataset2[\"text\"] = dataset2[\"text\"].apply(clean_text)\n",
    "dataset2[\"title\"] = dataset2[\"title\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'title' and 'text' columns into a new column 'combined'\n",
    "dataset2['combined'] = dataset2['title'] + ' ' + dataset2['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'text', 'subject', 'label', 'combined'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as us budget fight looms republicans flip thei...</td>\n",
       "      <td>washington reuters the head of a conservative ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>as us budget fight looms republicans flip thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us military to accept transgender recruits on ...</td>\n",
       "      <td>washington reuters transgender people will be ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>us military to accept transgender recruits on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senior us republican senator let mr mueller do...</td>\n",
       "      <td>washington reuters the special counsel investi...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>senior us republican senator let mr mueller do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "      <td>washington reuters trump campaign adviser geor...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump wants postal service to charge much more...</td>\n",
       "      <td>seattlewashington reuters president donald tru...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>trump wants postal service to charge much more...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  as us budget fight looms republicans flip thei...   \n",
       "1  us military to accept transgender recruits on ...   \n",
       "2  senior us republican senator let mr mueller do...   \n",
       "3  fbi russia probe helped by australian diplomat...   \n",
       "4  trump wants postal service to charge much more...   \n",
       "\n",
       "                                                text       subject  label  \\\n",
       "0  washington reuters the head of a conservative ...  politicsNews      1   \n",
       "1  washington reuters transgender people will be ...  politicsNews      1   \n",
       "2  washington reuters the special counsel investi...  politicsNews      1   \n",
       "3  washington reuters trump campaign adviser geor...  politicsNews      1   \n",
       "4  seattlewashington reuters president donald tru...  politicsNews      1   \n",
       "\n",
       "                                            combined  \n",
       "0  as us budget fight looms republicans flip thei...  \n",
       "1  us military to accept transgender recruits on ...  \n",
       "2  senior us republican senator let mr mueller do...  \n",
       "3  fbi russia probe helped by australian diplomat...  \n",
       "4  trump wants postal service to charge much more...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset2.columns)\n",
    "dataset2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the tokenizer models if not already downloaded\n",
    "\n",
    "# Tokenize the combined text\n",
    "dataset2['tokens'] = dataset2['combined'].apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
    "\n",
    "# Define the set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokens\n",
    "dataset2['tokens'] = dataset2['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "dataset2['stemmed'] = dataset2['tokens'].apply(lambda tokens: [ps.stem(word) for word in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokens in the 'stemmed' column into a single string\n",
    "dataset2['stemmed_text'] = dataset2['stemmed'].apply(lambda tokens: ' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit vocabulary to the top 5000 most frequent words\n",
    "vectorizer = CountVectorizer(max_features=10000, min_df=5, max_df=0.8)\n",
    "vector1 = vectorizer.fit_transform(dataset2['stemmed_text'])\n",
    "\n",
    "\n",
    "# Convert the sparse matrix to a dense array and store it in a new column\n",
    "dataset2['combined_text_encoded'] = vector1.toarray().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset2.drop(columns=['tokens', 'stemmed','stemmed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero entries: 6401272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>combined</th>\n",
       "      <th>combined_text_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as us budget fight looms republicans flip thei...</td>\n",
       "      <td>washington reuters the head of a conservative ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>as us budget fight looms republicans flip thei...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us military to accept transgender recruits on ...</td>\n",
       "      <td>washington reuters transgender people will be ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>us military to accept transgender recruits on ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senior us republican senator let mr mueller do...</td>\n",
       "      <td>washington reuters the special counsel investi...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>senior us republican senator let mr mueller do...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "      <td>washington reuters trump campaign adviser geor...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump wants postal service to charge much more...</td>\n",
       "      <td>seattlewashington reuters president donald tru...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "      <td>trump wants postal service to charge much more...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  as us budget fight looms republicans flip thei...   \n",
       "1  us military to accept transgender recruits on ...   \n",
       "2  senior us republican senator let mr mueller do...   \n",
       "3  fbi russia probe helped by australian diplomat...   \n",
       "4  trump wants postal service to charge much more...   \n",
       "\n",
       "                                                text       subject  label  \\\n",
       "0  washington reuters the head of a conservative ...  politicsNews      1   \n",
       "1  washington reuters transgender people will be ...  politicsNews      1   \n",
       "2  washington reuters the special counsel investi...  politicsNews      1   \n",
       "3  washington reuters trump campaign adviser geor...  politicsNews      1   \n",
       "4  seattlewashington reuters president donald tru...  politicsNews      1   \n",
       "\n",
       "                                            combined  \\\n",
       "0  as us budget fight looms republicans flip thei...   \n",
       "1  us military to accept transgender recruits on ...   \n",
       "2  senior us republican senator let mr mueller do...   \n",
       "3  fbi russia probe helped by australian diplomat...   \n",
       "4  trump wants postal service to charge much more...   \n",
       "\n",
       "                               combined_text_encoded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nonzero_count = vector1.nnz  # or X.count_nonzero()\n",
    "print(\"Number of nonzero entries:\", nonzero_count)\n",
    "\n",
    "\n",
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Luisa & Max please try the code below: \n",
    "\n",
    "(This cell to be deleted if it does not work for none of us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vector = vectorizer.fit_transform(dataset2['stemmed_text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense array and store it in a new column\n",
    "dataset2['combined_text_encoded'] = vector.toarray().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9164490861618799\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.90      0.93      0.92       558\n",
      "        REAL       0.93      0.91      0.92       591\n",
      "\n",
      "    accuracy                           0.92      1149\n",
      "   macro avg       0.92      0.92      0.92      1149\n",
      "weighted avg       0.92      0.92      0.92      1149\n",
      "\n",
      "Naive Bayes Accuracy: 0.8833768494342907\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.89      0.87      0.88       558\n",
      "        REAL       0.88      0.89      0.89       591\n",
      "\n",
      "    accuracy                           0.88      1149\n",
      "   macro avg       0.88      0.88      0.88      1149\n",
      "weighted avg       0.88      0.88      0.88      1149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix (X) and target vector (y) for model training\n",
    "X = dataset1['combined_text_encoded'].tolist()\n",
    "y = dataset1['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "# Train and evaluate a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))\n",
    "\n",
    "# Train and evaluate a Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb, target_names=['FAKE', 'REAL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9978619153674833\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       1.00      1.00      1.00      5876\n",
      "        REAL       1.00      1.00      1.00      5349\n",
      "\n",
      "    accuracy                           1.00     11225\n",
      "   macro avg       1.00      1.00      1.00     11225\n",
      "weighted avg       1.00      1.00      1.00     11225\n",
      "\n",
      "Naive Bayes Accuracy: 0.9455679287305122\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.95      0.95      0.95      5876\n",
      "        REAL       0.94      0.94      0.94      5349\n",
      "\n",
      "    accuracy                           0.95     11225\n",
      "   macro avg       0.95      0.95      0.95     11225\n",
      "weighted avg       0.95      0.95      0.95     11225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix (X) and target vector (y) for model training\n",
    "X = dataset2['combined_text_encoded'].tolist()\n",
    "y = dataset2['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "# Train and evaluate a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))\n",
    "\n",
    "# Train and evaluate a Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb, target_names=['FAKE', 'REAL']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
