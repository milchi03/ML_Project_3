{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Classifier as benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this exercise is to get a feeling and understanding on the importance of\n",
    "representation and extraction of information from complex media content, in this case images or\n",
    "text. You will thus get some datasets that have an image classification target.  \n",
    "\n",
    "(1) In the first step, you shall try to find a good classifier with „traditional“ feature extraction\n",
    "methods. Thus, pick one feature extractor based on e.g. Bag Of Words, or n-grams, or similar\n",
    "You shall evaluate them on two shallow algorithms, optimising the parameter settings to see what\n",
    "performance you can achieve, to have a baseline for the subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"Data/fake_and_real_news_dataset.csv\" \n",
    "dataset1 = pd.read_csv(file_path, encoding=\"utf-8\", on_bad_lines='skip')\n",
    "\n",
    "\n",
    "dataset1.columns = ['iid', 'title', 'text', 'label']\n",
    "\n",
    "print(dataset1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in the 'label' column with 'FAKE'\n",
    "dataset1['label'].fillna('FAKE', inplace=True)\n",
    "\n",
    "# Verify that there are no missing values left in the 'label' column\n",
    "print(dataset1['label'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          iid                                              title  \\\n",
      "0  Fq+C96tcx+  ‘A target on Roe v. Wade ’: Oklahoma bill maki...   \n",
      "1  bHUqK!pgmv  Study: women had to drive 4 times farther afte...   \n",
      "2  4Y4Ubf%aTi        Trump, Clinton clash in dueling DC speeches   \n",
      "3  _CoY89SJ@K  Grand jury in Texas indicts activists behind P...   \n",
      "4  +rJHoRQVLe  As Reproductive Rights Hang In The Balance, De...   \n",
      "\n",
      "                                                text  label  \\\n",
      "0  UPDATE: Gov. Fallin vetoed the bill on Friday....      1   \n",
      "1  Ever since Texas laws closed about half of the...      1   \n",
      "2  Donald Trump and Hillary Clinton, now at the s...      1   \n",
      "3  A Houston grand jury investigating criminal al...      1   \n",
      "4  WASHINGTON -- Forty-three years after the Supr...      1   \n",
      "\n",
      "                                       combined_text  \\\n",
      "0   a target on roe v wade oklahoma bill making i...   \n",
      "1  study women had to drive 4 times farther after...   \n",
      "2  trump clinton clash in dueling dc speeches don...   \n",
      "3  grand jury in texas indicts activists behind p...   \n",
      "4  as reproductive rights hang in the balance deb...   \n",
      "\n",
      "                               combined_text_encoded  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Encode labels: FAKE -> 0, TRUE -> 1\n",
    "dataset1['label'] = dataset1['label'].map({'FAKE': 0, 'REAL': 1})\n",
    "\n",
    "# Combine 'title' and 'text' into one column\n",
    "dataset1['combined_text'] = dataset1['title'] + \" \" + dataset1['text']\n",
    "\n",
    "# Convert text to lowercase\n",
    "dataset1['combined_text'] = dataset1['combined_text'].str.lower()\n",
    "\n",
    "# Remove special characters and punctuation\n",
    "dataset1['combined_text'] = dataset1['combined_text'].apply(lambda x: re.sub(r'\\W+', ' ', str(x)))\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))  # Define stop words\n",
    "\n",
    "# Tokenization, stop-word removal, and stemming\n",
    "dataset1['combined_text_tokens'] = dataset1['combined_text'].apply(word_tokenize)\n",
    "dataset1['combined_text_tokens'] = dataset1['combined_text_tokens'].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
    ")\n",
    "dataset1['combined_text_stemmed'] = dataset1['combined_text_tokens'].apply(\n",
    "    lambda tokens: [ps.stem(token) for token in tokens]\n",
    ")\n",
    "\n",
    "# Convert stemmed tokens back into strings for CountVectorizer\n",
    "dataset1['combined_text_stemmed_text'] = dataset1['combined_text_stemmed'].apply(' '.join)\n",
    "\n",
    "# Use CountVectorizer to convert text into a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(dataset1['combined_text_stemmed_text'])\n",
    "dataset1['combined_text_encoded'] = vector.toarray().tolist()\n",
    "\n",
    "# Drop intermediate columns\n",
    "dataset1 = dataset1.drop(columns=['combined_text_tokens', 'combined_text_stemmed', 'combined_text_stemmed_text'])\n",
    "\n",
    "\n",
    "\n",
    "print(dataset1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the combined_text_encoded column we can only see zeros we will check if there are non-zero values in order to be sure the preprocessing has gone smoothly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero entries: 1226720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>combined_text_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fq+C96tcx+</td>\n",
       "      <td>‘A target on Roe v. Wade ’: Oklahoma bill maki...</td>\n",
       "      <td>UPDATE: Gov. Fallin vetoed the bill on Friday....</td>\n",
       "      <td>1</td>\n",
       "      <td>a target on roe v wade oklahoma bill making i...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bHUqK!pgmv</td>\n",
       "      <td>Study: women had to drive 4 times farther afte...</td>\n",
       "      <td>Ever since Texas laws closed about half of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>study women had to drive 4 times farther after...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Y4Ubf%aTi</td>\n",
       "      <td>Trump, Clinton clash in dueling DC speeches</td>\n",
       "      <td>Donald Trump and Hillary Clinton, now at the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>trump clinton clash in dueling dc speeches don...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_CoY89SJ@K</td>\n",
       "      <td>Grand jury in Texas indicts activists behind P...</td>\n",
       "      <td>A Houston grand jury investigating criminal al...</td>\n",
       "      <td>1</td>\n",
       "      <td>grand jury in texas indicts activists behind p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+rJHoRQVLe</td>\n",
       "      <td>As Reproductive Rights Hang In The Balance, De...</td>\n",
       "      <td>WASHINGTON -- Forty-three years after the Supr...</td>\n",
       "      <td>1</td>\n",
       "      <td>as reproductive rights hang in the balance deb...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          iid                                              title  \\\n",
       "0  Fq+C96tcx+  ‘A target on Roe v. Wade ’: Oklahoma bill maki...   \n",
       "1  bHUqK!pgmv  Study: women had to drive 4 times farther afte...   \n",
       "2  4Y4Ubf%aTi        Trump, Clinton clash in dueling DC speeches   \n",
       "3  _CoY89SJ@K  Grand jury in Texas indicts activists behind P...   \n",
       "4  +rJHoRQVLe  As Reproductive Rights Hang In The Balance, De...   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  UPDATE: Gov. Fallin vetoed the bill on Friday....      1   \n",
       "1  Ever since Texas laws closed about half of the...      1   \n",
       "2  Donald Trump and Hillary Clinton, now at the s...      1   \n",
       "3  A Houston grand jury investigating criminal al...      1   \n",
       "4  WASHINGTON -- Forty-three years after the Supr...      1   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0   a target on roe v wade oklahoma bill making i...   \n",
       "1  study women had to drive 4 times farther after...   \n",
       "2  trump clinton clash in dueling dc speeches don...   \n",
       "3  grand jury in texas indicts activists behind p...   \n",
       "4  as reproductive rights hang in the balance deb...   \n",
       "\n",
       "                               combined_text_encoded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nonzero_count = vector.nnz  # or X.count_nonzero()\n",
    "print(\"Number of nonzero entries:\", nonzero_count)\n",
    "\n",
    "\n",
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  label  \n",
      "0  December 31, 2017       1  \n",
      "1  December 29, 2017       1  \n",
      "2  December 31, 2017       1  \n",
      "3  December 30, 2017       1  \n",
      "4  December 29, 2017       1  \n"
     ]
    }
   ],
   "source": [
    "true_df = pd.read_csv(\"Data/True.csv\")\n",
    "fake_df = pd.read_csv(\"Data/Fake.csv\")\n",
    "\n",
    "# Add label column\n",
    "true_df[\"label\"] = 1  \n",
    "fake_df[\"label\"] = 0  \n",
    "\n",
    "# Combine both datasets\n",
    "dataset2 = pd.concat([true_df, fake_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(dataset2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the 'date' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset2.drop(columns=[\"date\", \"subject\"])\n",
    "dataset2 = dataset2.sample(n = 10000, random_state= 420)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset2.isnull().sum())  # Check missing values\n",
    "dataset2 = dataset2.dropna()  # Drop rows with missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning (converting text to lowercase, removing special characters, numbers and punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'title' and 'text' columns into a new column 'combined'\n",
    "dataset2['combined'] = dataset2['title'] + ' ' + dataset2['text']\n",
    "# Convert text to lowercase\n",
    "dataset2['combined'] = dataset2['combined'].str.lower()\n",
    "\n",
    "# Remove special characters and punctuation\n",
    "dataset2['combined'] = dataset2['combined'].apply(lambda x: re.sub(r'\\W+', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'text', 'label', 'combined'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37060</th>\n",
       "      <td>VATICAN ADVISOR: Says Pope Will Call On World ...</td>\n",
       "      <td>Members of the Catholic church need to pay clo...</td>\n",
       "      <td>0</td>\n",
       "      <td>vatican advisor says pope will call on world a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33184</th>\n",
       "      <td>WOW! REFUGEES EXPOSED: Here’s the cold hard tr...</td>\n",
       "      <td>Brigitte Gabriel is an intelligent and importa...</td>\n",
       "      <td>0</td>\n",
       "      <td>wow refugees exposed here s the cold hard trut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>BUSTED! DEM TX REP PLAYS RACE CARD, LIES ABOUT...</td>\n",
       "      <td>The race card thing is getting old fast The Au...</td>\n",
       "      <td>0</td>\n",
       "      <td>busted dem tx rep plays race card lies about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42649</th>\n",
       "      <td>ATHEIST TEACHER Gets 8 Year Old One-Week Suspe...</td>\n",
       "      <td>Bullies come in all different shapes, sizes an...</td>\n",
       "      <td>0</td>\n",
       "      <td>atheist teacher gets 8 year old one week suspe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39351</th>\n",
       "      <td>FLASHBACK: Mark Steyn: Why The US Is Becoming ...</td>\n",
       "      <td>Mark Steyn is dead on when he calls out the fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>flashback mark steyn why the us is becoming a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "37060  VATICAN ADVISOR: Says Pope Will Call On World ...   \n",
       "33184  WOW! REFUGEES EXPOSED: Here’s the cold hard tr...   \n",
       "36763  BUSTED! DEM TX REP PLAYS RACE CARD, LIES ABOUT...   \n",
       "42649  ATHEIST TEACHER Gets 8 Year Old One-Week Suspe...   \n",
       "39351  FLASHBACK: Mark Steyn: Why The US Is Becoming ...   \n",
       "\n",
       "                                                    text  label  \\\n",
       "37060  Members of the Catholic church need to pay clo...      0   \n",
       "33184  Brigitte Gabriel is an intelligent and importa...      0   \n",
       "36763  The race card thing is getting old fast The Au...      0   \n",
       "42649  Bullies come in all different shapes, sizes an...      0   \n",
       "39351  Mark Steyn is dead on when he calls out the fe...      0   \n",
       "\n",
       "                                                combined  \n",
       "37060  vatican advisor says pope will call on world a...  \n",
       "33184  wow refugees exposed here s the cold hard trut...  \n",
       "36763  busted dem tx rep plays race card lies about t...  \n",
       "42649  atheist teacher gets 8 year old one week suspe...  \n",
       "39351  flashback mark steyn why the us is becoming a ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset2.columns)\n",
    "dataset2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the tokenizer models if not already downloaded\n",
    "\n",
    "# Tokenize the combined text\n",
    "dataset2['tokens'] = dataset2['combined'].apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
    "\n",
    "# Define the set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokens\n",
    "dataset2['tokens'] = dataset2['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "dataset2['stemmed'] = dataset2['tokens'].apply(lambda tokens: [ps.stem(word) for word in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokens in the 'stemmed' column into a single string\n",
    "dataset2['stemmed_text'] = dataset2['stemmed'].apply(lambda tokens: ' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vector = vectorizer.fit_transform(dataset2['stemmed_text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense array and store it in a new column\n",
    "dataset2['combined_text_encoded'] = vector.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset2.drop(columns=['tokens', 'stemmed','stemmed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero entries: 1499507\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined</th>\n",
       "      <th>combined_text_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37060</th>\n",
       "      <td>VATICAN ADVISOR: Says Pope Will Call On World ...</td>\n",
       "      <td>Members of the Catholic church need to pay clo...</td>\n",
       "      <td>0</td>\n",
       "      <td>vatican advisor says pope will call on world a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33184</th>\n",
       "      <td>WOW! REFUGEES EXPOSED: Here’s the cold hard tr...</td>\n",
       "      <td>Brigitte Gabriel is an intelligent and importa...</td>\n",
       "      <td>0</td>\n",
       "      <td>wow refugees exposed here s the cold hard trut...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>BUSTED! DEM TX REP PLAYS RACE CARD, LIES ABOUT...</td>\n",
       "      <td>The race card thing is getting old fast The Au...</td>\n",
       "      <td>0</td>\n",
       "      <td>busted dem tx rep plays race card lies about t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42649</th>\n",
       "      <td>ATHEIST TEACHER Gets 8 Year Old One-Week Suspe...</td>\n",
       "      <td>Bullies come in all different shapes, sizes an...</td>\n",
       "      <td>0</td>\n",
       "      <td>atheist teacher gets 8 year old one week suspe...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39351</th>\n",
       "      <td>FLASHBACK: Mark Steyn: Why The US Is Becoming ...</td>\n",
       "      <td>Mark Steyn is dead on when he calls out the fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>flashback mark steyn why the us is becoming a ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "37060  VATICAN ADVISOR: Says Pope Will Call On World ...   \n",
       "33184  WOW! REFUGEES EXPOSED: Here’s the cold hard tr...   \n",
       "36763  BUSTED! DEM TX REP PLAYS RACE CARD, LIES ABOUT...   \n",
       "42649  ATHEIST TEACHER Gets 8 Year Old One-Week Suspe...   \n",
       "39351  FLASHBACK: Mark Steyn: Why The US Is Becoming ...   \n",
       "\n",
       "                                                    text  label  \\\n",
       "37060  Members of the Catholic church need to pay clo...      0   \n",
       "33184  Brigitte Gabriel is an intelligent and importa...      0   \n",
       "36763  The race card thing is getting old fast The Au...      0   \n",
       "42649  Bullies come in all different shapes, sizes an...      0   \n",
       "39351  Mark Steyn is dead on when he calls out the fe...      0   \n",
       "\n",
       "                                                combined  \\\n",
       "37060  vatican advisor says pope will call on world a...   \n",
       "33184  wow refugees exposed here s the cold hard trut...   \n",
       "36763  busted dem tx rep plays race card lies about t...   \n",
       "42649  atheist teacher gets 8 year old one week suspe...   \n",
       "39351  flashback mark steyn why the us is becoming a ...   \n",
       "\n",
       "                                   combined_text_encoded  \n",
       "37060  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "33184  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "36763  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "42649  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39351  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_count = vector1.nnz  # or X.count_nonzero()\n",
    "print(\"Number of nonzero entries:\", nonzero_count)\n",
    "\n",
    "\n",
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9016536118363795\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.89      0.92      0.90       560\n",
      "        REAL       0.92      0.89      0.90       589\n",
      "\n",
      "    accuracy                           0.90      1149\n",
      "   macro avg       0.90      0.90      0.90      1149\n",
      "weighted avg       0.90      0.90      0.90      1149\n",
      "\n",
      "Naive Bayes Accuracy: 0.8955613577023499\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.92      0.86      0.89       560\n",
      "        REAL       0.88      0.93      0.90       589\n",
      "\n",
      "    accuracy                           0.90      1149\n",
      "   macro avg       0.90      0.89      0.90      1149\n",
      "weighted avg       0.90      0.90      0.90      1149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix (X) and target vector (y) for model training\n",
    "X = dataset1['combined_text_encoded'].tolist()\n",
    "y = dataset1['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=609)\n",
    "\n",
    "# Train and evaluate a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))\n",
    "\n",
    "# Train and evaluate a Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb, target_names=['FAKE', 'REAL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9852\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.99      0.98      0.99      1279\n",
      "        REAL       0.98      0.99      0.98      1221\n",
      "\n",
      "    accuracy                           0.99      2500\n",
      "   macro avg       0.99      0.99      0.99      2500\n",
      "weighted avg       0.99      0.99      0.99      2500\n",
      "\n",
      "Naive Bayes Accuracy: 0.9484\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.95      0.95      0.95      1279\n",
      "        REAL       0.94      0.95      0.95      1221\n",
      "\n",
      "    accuracy                           0.95      2500\n",
      "   macro avg       0.95      0.95      0.95      2500\n",
      "weighted avg       0.95      0.95      0.95      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix (X) and target vector (y) for model training\n",
    "X = dataset2['combined_text_encoded'].tolist()\n",
    "y = dataset2['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "\n",
    "# Train and evaluate a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))\n",
    "\n",
    "# Train and evaluate a Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb, target_names=['FAKE', 'REAL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "glove_twitter = \"Data/glove.twitter.27B.100d.txt\" #from https://www.kaggle.com/datasets/icw123/glove-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'rspit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17648\\1605600543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_coeffs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrspit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_twitter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17648\\1605600543.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_coeffs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrspit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_twitter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'rspit'"
     ]
    }
   ],
   "source": [
    "def get_coeffs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype = \"float32\")\n",
    "\n",
    "embeddings_index = dict(get_coeffs(*g.rstrip().rspit(\" \")) for g in open(glove_twitter))\n",
    "embedding_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
