{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Classifier as benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this exercise is to get a feeling and understanding on the importance of\n",
    "representation and extraction of information from complex media content, in this case images or\n",
    "text. You will thus get some datasets that have an image classification target.  \n",
    "\n",
    "(1) In the first step, you shall try to find a good classifier with „traditional“ feature extraction\n",
    "methods. Thus, pick one feature extractor based on e.g. Bag Of Words, or n-grams, or similar\n",
    "You shall evaluate them on two shallow algorithms, optimising the parameter settings to see what\n",
    "performance you can achieve, to have a baseline for the subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame:\n",
      "                                               title  \\\n",
      "0  In public, they ran a successful public busine...   \n",
      "1  With tariffs signed, Trump warns of pain to co...   \n",
      "2  Even a potential ceasefire sparks little hope ...   \n",
      "3  How the Trump administration is building out i...   \n",
      "\n",
      "                                  title_stemmed_text  \n",
      "0  in public , they ran a success public busi . i...  \n",
      "1  with tariff sign , trump warn of pain to come ...  \n",
      "2  even a potenti ceasefir spark littl hope in ea...  \n",
      "3  how the trump administr is build out it immigr...  \n",
      "\n",
      "Vocabulary:  {'in': 16, 'public': 25, 'they': 32, 'ran': 26, 'success': 29, 'busi': 5, 'alleg': 2, 'abus': 0, 'hundr': 14, 'of': 21, 'with': 37, 'tariff': 30, 'sign': 27, 'trump': 34, 'warn': 36, 'pain': 23, 'to': 33, 'come': 7, 'for': 11, 'american': 3, 'even': 10, 'potenti': 24, 'ceasefir': 6, 'spark': 28, 'littl': 19, 'hope': 12, 'eastern': 8, 'ukrain': 35, 'how': 13, 'the': 31, 'administr': 1, 'is': 17, 'build': 4, 'out': 22, 'it': 18, 'immigr': 15, 'enforc': 9, 'machin': 20}\n",
      "\n",
      "Encoded Document is:\n",
      "[[1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 1 0 0 0 5 1 0 0 1 0 0 2 0 0 0\n",
      "  0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0\n",
      "  1 1]\n",
      " [0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      "  0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample dataset\n",
    "dataset1 = pd.DataFrame({\n",
    "    'iid': ['KS823', 'PA432', 'BH235', 'HD732'],\n",
    "    'title': [\n",
    "        'In public, they ran a successful public business. In public, public allege they abused hundreds of public',\n",
    "        'With tariffs signed, Trump warns of pain to come for Americans',\n",
    "        'Even a potential ceasefire sparks little hope in eastern Ukraine',\n",
    "        'How the Trump administration is building out its immigration enforcement machine'\n",
    "    ],\n",
    "    'lable': ['FAKE', 'FAKE', 'TRUE', 'TRUE'],\n",
    "})\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize the 'title' column\n",
    "dataset1['title_tokens'] = dataset1['title'].apply(word_tokenize)\n",
    "\n",
    "# Define a function to stem a list of tokens\n",
    "def stem_tokens(tokens):\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "# Apply the stem_tokens function to the tokenized column\n",
    "dataset1['title_stemmed'] = dataset1['title_tokens'].apply(stem_tokens)\n",
    "\n",
    "# Convert stemmed tokens back into strings for CountVectorizer\n",
    "dataset1['title_stemmed_text'] = dataset1['title_stemmed'].apply(' '.join)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Processed DataFrame:\")\n",
    "print(dataset1[['title', 'title_stemmed_text']])\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the stemmed text\n",
    "vectorizer.fit(dataset1['title_stemmed_text'])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"\\nVocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the documents\n",
    "vector = vectorizer.transform(dataset1['title_stemmed_text'])\n",
    "\n",
    "# Summarize the encoded texts\n",
    "print(\"\\nEncoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
